{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"mount_file_id":"1YKA4-1mB4m5uVqWHU_Uuep6QBD24x4Ye","authorship_tag":"ABX9TyPqTSNDv9BuCHgfo6zocCaC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IT1DJWW_L5or","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615543903551,"user_tz":-330,"elapsed":47904,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}},"outputId":"67390725-ba15-42c4-cdd0-0ab68f03909b"},"source":["from google.colab import drive\r\n","\r\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DkYMptKOLbFb"},"source":["import pandas as pd\r\n","import json\r\n","from sklearn.model_selection import train_test_split\r\n","df=pd.read_json('/content/gdrive/MyDrive/archive/data2.gz', lines=True,compression='gzip',)    #data reading from drive\r\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WfLZyQv9JRPc","executionInfo":{"status":"ok","timestamp":1615544053335,"user_tz":-330,"elapsed":1261,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}}},"source":["df=df.iloc[:1000]                                        # I have taken 1000 samples for training \r\n","data_new=df[['book_id','review_text']]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"pKxB73V3J-Tq","executionInfo":{"status":"ok","timestamp":1615544233337,"user_tz":-330,"elapsed":1351,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}},"outputId":"f5b37896-1473-4c24-a47d-ad0108734048"},"source":["data=data_new.set_index('book_id')\r\n","data"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_text</th>\n","    </tr>\n","    <tr>\n","      <th>book_id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>23310161</th>\n","      <td>Fun sequel to the original.</td>\n","    </tr>\n","    <tr>\n","      <th>17290220</th>\n","      <td>One of my favorite books to read to my 5 year ...</td>\n","    </tr>\n","    <tr>\n","      <th>6954929</th>\n","      <td>One of the best and most imaginative childrens...</td>\n","    </tr>\n","    <tr>\n","      <th>460548</th>\n","      <td>My daughter is loving this. Published in the 6...</td>\n","    </tr>\n","    <tr>\n","      <th>11474551</th>\n","      <td>A friend sent me this. Hilarious!</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>17063690</th>\n","      <td>There was a mix-up in the puppy department! On...</td>\n","    </tr>\n","    <tr>\n","      <th>18475599</th>\n","      <td>A little clown falls off the circus train and ...</td>\n","    </tr>\n","    <tr>\n","      <th>20518948</th>\n","      <td>Who knew there existed a bilingual biography f...</td>\n","    </tr>\n","    <tr>\n","      <th>20696727</th>\n","      <td>Interesting history of Roget and his Thesaurus...</td>\n","    </tr>\n","    <tr>\n","      <th>14950010</th>\n","      <td>This is one of the most fun children's poetry ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows Ã— 1 columns</p>\n","</div>"],"text/plain":["                                                review_text\n","book_id                                                    \n","23310161                        Fun sequel to the original.\n","17290220  One of my favorite books to read to my 5 year ...\n","6954929   One of the best and most imaginative childrens...\n","460548    My daughter is loving this. Published in the 6...\n","11474551                  A friend sent me this. Hilarious!\n","...                                                     ...\n","17063690  There was a mix-up in the puppy department! On...\n","18475599  A little clown falls off the circus train and ...\n","20518948  Who knew there existed a bilingual biography f...\n","20696727  Interesting history of Roget and his Thesaurus...\n","14950010  This is one of the most fun children's poetry ...\n","\n","[1000 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wLBEr_5XoZv","executionInfo":{"status":"ok","timestamp":1615544405876,"user_tz":-330,"elapsed":1311,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}},"outputId":"2c5bcb59-b756-44f5-f5e3-155195668597"},"source":["import nltk \r\n","nltk.download('punkt') \r\n","nltk.download('averaged_perceptron_tagger') \r\n","nltk.download('wordnet') \r\n","  \r\n","  \r\n","from nltk.stem import WordNetLemmatizer \r\n","lemmatizer = WordNetLemmatizer() \r\n","  \r\n","from nltk.corpus import stopwords \r\n","nltk.download('stopwords') \r\n","stop_words = set(stopwords.words('english')) \r\n","  \r\n","VERB_CODES = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'} "],"execution_count":12,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MX64CtCFXtf8","executionInfo":{"status":"ok","timestamp":1615544967397,"user_tz":-330,"elapsed":4270,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}}},"source":["def preprocess_sentences(text): \r\n","  text = text.lower() \r\n","  temp_sent =[] \r\n","  words = nltk.word_tokenize(text) \r\n","  tags = nltk.pos_tag(words) \r\n","  for i, word in enumerate(words): \r\n","      if tags[i][1] in VERB_CODES:  \r\n","          lemmatized = lemmatizer.lemmatize(word, 'v') \r\n","      else: \r\n","          lemmatized = lemmatizer.lemmatize(word) \r\n","      if lemmatized not in stop_words and lemmatized.isalpha(): \r\n","          temp_sent.append(lemmatized) \r\n","          \r\n","  datasent = ' '.join(temp_sent) \r\n","  datasent = datasent.replace(\"n't\", \" not\") \r\n","  datasent = datasent.replace(\"'m\", \" am\") \r\n","  datasent = datasent.replace(\"'s\", \" is\") \r\n","  datasent = datasent.replace(\"'re\", \" are\") \r\n","  datasent = datasent.replace(\"'ll\", \" will\") \r\n","  datasent = datasent.replace(\"'ve\", \" have\") \r\n","  datasent = datasent.replace(\"'d\", \" would\") \r\n","  return datasent \r\n","  \r\n","data[\"text_processed\"]= data[\"review_text\"].apply(preprocess_sentences) \r\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofUfoEY6a3y8","executionInfo":{"status":"ok","timestamp":1615545048862,"user_tz":-330,"elapsed":1212,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer \r\n","  \r\n","# Vectorizing pre-processed movie plots using TF-IDF \r\n","tfidfvec = TfidfVectorizer() \r\n","tfidf_bookid = tfidfvec.fit_transform((data[\"text_processed\"])) \r\n","  \r\n","# Finding cosine similarity between vectors \r\n","from sklearn.metrics.pairwise import cosine_similarity \r\n","cos_sim = cosine_similarity(tfidf_bookid, tfidf_bookid) "],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c13VzfH4dcbR","executionInfo":{"status":"ok","timestamp":1615545052485,"user_tz":-330,"elapsed":1256,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}},"outputId":"433cd001-0dc2-497b-e682-ea1b03b13493"},"source":["cos_sim.shape"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 1000)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"Za782CpXbLx9","executionInfo":{"status":"ok","timestamp":1615544615688,"user_tz":-330,"elapsed":1272,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}}},"source":["indices = pd.Series(data.index) \r\n","  \r\n","def recommendations(book_id, cosine_sim = cos_sim): \r\n","    recommended_books = [] \r\n","    index = indices[indices == book_id].index[0] \r\n","    similarity_scores = pd.Series(cosine_sim[index]).sort_values(ascending = False) \r\n","    top_3_books = list(similarity_scores.iloc[1:4].index) \r\n","    for i in top_3_books: \r\n","        recommended_books.append(list(data.index)[i]) \r\n","    return recommended_books "],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rOglsaBxbuEC","executionInfo":{"status":"ok","timestamp":1615545061381,"user_tz":-330,"elapsed":1435,"user":{"displayName":"aparna singh 19210071","photoUrl":"","userId":"12912834262738368773"}},"outputId":"f475423a-2a99-4265-ed29-ebc7e22d29d8"},"source":["recommendations(17290220)            #recommendation of three books with resulted book_idswho have read the book with book_id 17290220"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[20454095, 1265468, 304889]"]},"metadata":{"tags":[]},"execution_count":28}]}]}